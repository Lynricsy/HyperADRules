name: Process Domain Lists

on:
  push:
    branches:
      - main
  workflow_dispatch:
  schedule:
    - cron: '0 * * * *'  # 每小时触发一次

jobs:
  process_domains:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v3

    - name: Set Up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install Dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y curl

    - name: Download and Process Domain Lists
      run: |
        #!/bin/bash

        # Define the URLs
        urls=(
          "https://raw.githubusercontent.com/Cats-Team/AdRules/script/script/allowlist.txt"
          "https://raw.githubusercontent.com/privacy-protection-tools/dead-horse/master/anti-ad-white-list.txt"
          # Add other URLs here if needed
        )

        # Step 1: Download and process each URL
        for url in "${urls[@]}"; do
          filename=$(basename "$url")

          # Extract lines with || prefix, pure domain, and $ suffix
          curl -s "$url" | awk '/\|\|[a-zA-Z0-9.-]+\$/' > "special_${filename}"
        done

        # Merge special domain files into special_domains.txt
        cat special_*.txt | sort -u > special_domains.txt

        # Step 2: Create allow.txt with @@||...^ format
        awk '{ print "@@||" $0 "^" }' special_domains.txt > allow.txt

        # Create the target directory if it does not exist
        mkdir -p data/rules

        # Move the generated files to the processed folder
        mv allow.txt data/rules/
        mv special_domains.txt data/rules/

    - name: Commit and Push Changes
      uses: EndBug/add-and-commit@v9
      with:
        add: |
          data/rules/allow.txt
          data/rules/special_domains.txt
        message: 'Update domain rules with new data'
